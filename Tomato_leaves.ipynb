{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9NuI543PqG5oCTpoH4tQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek980-tech/Agro-Vision-Tomato_Leaves_disease_detection-/blob/main/Tomato_leaves.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Installation**"
      ],
      "metadata": {
        "id": "QIYkjqE7Js5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1-5UmG1JOvj",
        "outputId": "d8bfbea9-c46d-4196-d435-906964bd37aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.1)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.0.6)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision timm albumentations opencv-python scikit-learn pandas matplotlib tqdm\n",
        "import os, shutil, random, gc, json, cv2\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import timm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Import**"
      ],
      "metadata": {
        "id": "4HKwqiL3Mua_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvG51tuhM8yo",
        "outputId": "021c98fb-a267-49e2-c00c-3920731359ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Initializing the path**"
      ],
      "metadata": {
        "id": "wgcpk4q1JeQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"/content/drive/MyDrive/Major_1/Agro_dataset/All_disease_of_Tomato_leaves\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/Major_1/Agro_dataset/output1\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "DUxIw1YBJZ6Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Utility Function**"
      ],
      "metadata": {
        "id": "WuZCZi85Jq9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_blurry(path, thresh=100):\n",
        "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: return True\n",
        "    return cv2.Laplacian(img, cv2.CV_64F).var() < thresh\n",
        "\n",
        "\n",
        "\n",
        "def build_metadata(root):\n",
        "    rows = []\n",
        "    root = Path(root)\n",
        "    for cls in sorted(p.name for p in root.iterdir() if p.is_dir()):\n",
        "        for f in (root/cls).glob('*'):\n",
        "            rows.append({'path': str(f), 'label': cls})\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "aI6HUWItDxEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Spliting of Data in the form of Training/Testing/Validation**"
      ],
      "metadata": {
        "id": "XtpUBzUeKUgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_splits(root, out, test_size=0.15, val_size=0.15, blur_check_fraction=0.1):\n",
        "    df = build_metadata(root)\n",
        "    counts = df['label'].value_counts()\n",
        "    valid_classes = counts[counts > 1].index\n",
        "    df = df[df['label'].isin(valid_classes)]\n",
        "    ok = []\n",
        "    for i, row in df.iterrows():\n",
        "        if not Path(row['path']).exists():\n",
        "            continue\n",
        "        if blur_check_fraction > 0 and random.random() < blur_check_fraction:\n",
        "            if is_blurry(row['path']):\n",
        "                continue\n",
        "        ok.append(row)\n",
        "\n",
        "    df_ok = pd.DataFrame(ok)\n",
        "\n",
        "\n",
        "    # Remove classes with <3 samples AFTER blur check\n",
        "    counts_ok = df_ok['label'].value_counts()\n",
        "    valid_classes_ok = counts_ok[counts_ok > 2].index\n",
        "    df_ok = df_ok[df_ok['label'].isin(valid_classes_ok)]\n",
        "\n",
        "\n",
        "    # Encode labels\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(df_ok['label'])\n",
        "\n",
        "\n",
        "    # Train / Rest split\n",
        "    try:\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=(test_size+val_size), random_state=42)\n",
        "        train_idx, rest_idx = next(sss.split(df_ok, y))\n",
        "    except ValueError:\n",
        "        print(\"Stratified split failed. Falling back to random split.\")\n",
        "        train_idx, rest_idx = train_test_split(range(len(df_ok)), test_size=(test_size+val_size), random_state=42)\n",
        "\n",
        "\n",
        "    df_train, df_rest = df_ok.iloc[train_idx], df_ok.iloc[rest_idx]\n",
        "\n",
        "\n",
        "    # Val / Test split\n",
        "    val_frac = val_size / (test_size + val_size)\n",
        "    y_rest = le.transform(df_rest['label'])\n",
        "    try:\n",
        "        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=1-val_frac, random_state=42)\n",
        "        val_idx, test_idx = next(sss2.split(df_rest, y_rest))\n",
        "    except ValueError:\n",
        "        print(\"Val/Test stratified split failed. Falling back to random split.\")\n",
        "        val_idx, test_idx = train_test_split(range(len(df_rest)), test_size=(1-val_frac), random_state=42)\n",
        "\n",
        "\n",
        "    df_val, df_test = df_rest.iloc[val_idx], df_rest.iloc[test_idx]\n",
        "\n",
        "\n",
        "    # Save CSVs\n",
        "    Path(out).mkdir(parents=True, exist_ok=True)\n",
        "    df_train.to_csv(Path(out)/\"train.csv\", index=False)\n",
        "    df_val.to_csv(Path(out)/\"val.csv\", index=False)\n",
        "    df_test.to_csv(Path(out)/\"test.csv\", index=False)\n",
        "\n",
        "\n",
        "    print(\"Train/Val/Test splits:\", len(df_train), len(df_val), len(df_test))\n",
        "    print(f\"Blur check applied on ~{blur_check_fraction*100}% of images\")\n",
        "\n",
        "\n",
        "prepare_splits(DATA_ROOT, OUT_DIR, blur_check_fraction=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERNO8WtBKSvh",
        "outputId": "72054dd4-02ab-4e42-d8e6-26f6efc14063"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val/Test stratified split failed. Falling back to random split.\n",
            "Train/Val/Test splits: 18056 3869 3870\n",
            "Blur check applied on ~10.0% of images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Dataset class**"
      ],
      "metadata": {
        "id": "0xpimz61PCBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TomatoDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_size=224, skip_bad=True):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.label2idx = {l: i for i, l in enumerate(sorted(self.df['label'].unique()))}\n",
        "        self.idx2label = {i: l for l, i in self.label2idx.items()}\n",
        "        self.skip_bad = skip_bad\n",
        "\n",
        "        self.T = A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.ColorJitter(p=0.5),\n",
        "            A.Normalize(),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "        if self.skip_bad:\n",
        "            good_rows = []\n",
        "            for i, row in self.df.iterrows():\n",
        "                if os.path.exists(row['path']) and cv2.imread(row['path']) is not None:\n",
        "                    good_rows.append(row)\n",
        "            self.df = pd.DataFrame(good_rows).reset_index(drop=True)\n",
        "            print(f\"✅ Dataset initialized with {len(self.df)} valid images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row['path']\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            if self.skip_bad:\n",
        "                return self.__getitem__((idx + 1) % len(self.df))\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"⚠️ Could not read image at {img_path}\")\n",
        "        img = img[:, :, ::-1]\n",
        "        img = self.T(image=img)['image']\n",
        "        label = self.label2idx[row['label']]\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "emf1urPRKpkJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Model Builder**"
      ],
      "metadata": {
        "id": "_AfnwGNcPKFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(backbone, num_classes):\n",
        "    if backbone == \"efficientnet_b0\":\n",
        "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "        in_features = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "    elif backbone == \"resnet50\":\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        in_features = model.fc.in_features\n",
        "        model.fc = nn.Linear(in_features, num_classes)\n",
        "    elif backbone == \"mobilenetv2_100\":\n",
        "        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
        "        in_features = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "    elif backbone == \"densenet121\":\n",
        "        model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        in_features = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(in_features, num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone: {backbone}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "J0IjHGVyKqrX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Training Function**"
      ],
      "metadata": {
        "id": "cjgQdll6PQvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(backbone, train_csv, val_csv, num_classes, epochs=20, batch_size=64, lr=1e-3):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    transform = T.Compose([\n",
        "        T.Resize((224,224)),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomRotation(20),\n",
        "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "    train_ds = TomatoDataset(train_csv)\n",
        "    val_ds   = TomatoDataset(val_csv)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    model = get_model(backbone, num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    history = {\"train_loss\":[], \"val_loss\":[], \"val_acc\":[]}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"[{backbone}] Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0,0,0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                preds = outputs.argmax(1)\n",
        "                correct += (preds==labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct/total\n",
        "\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"[{backbone}] Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "    return history, model\n"
      ],
      "metadata": {
        "id": "qX_jGDP1KvlW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Main Training loop**"
      ],
      "metadata": {
        "id": "d31SB4o3PXlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_csv = f\"{OUT_DIR}/train.csv\"\n",
        "val_csv   = f\"{OUT_DIR}/val.csv\"\n",
        "num_classes = len(pd.read_csv(train_csv)['label'].unique())\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "results = {}\n",
        "for backbone in [\"efficientnet_b0\",\"resnet50\",\"mobilenetv2_100\",\"densenet121\"]:\n",
        "    print(\"=\"*60)\n",
        "    print(f\" Training {backbone} \".center(60,\"=\"))\n",
        "    history, model = train_model(backbone, train_csv, val_csv, num_classes, epochs=20, batch_size=64)\n",
        "    results[backbone] = history\n",
        "    save_path = f\"models/{backbone}.pt\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"✅ Saved {backbone} weights to {save_path}\")\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbb4_pvtbPYD",
        "outputId": "38240162-c3ea-469f-db76-f9db6cc8c02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "================= Training efficientnet_b0 =================\n",
            "✅ Dataset initialized with 18054 valid images\n",
            "✅ Dataset initialized with 3869 valid images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[efficientnet_b0] Epoch 1/20: 100%|██████████| 283/283 [1:21:12<00:00, 17.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[efficientnet_b0] Epoch 1/20 | Train Loss: 0.5423 | Val Loss: 0.7613 | Val Acc: 0.8617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[efficientnet_b0] Epoch 2/20:   0%|          | 1/283 [00:31<2:27:41, 31.42s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Save training Loop**"
      ],
      "metadata": {
        "id": "WEcQi8hgPdOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame({b: pd.Series(h['val_acc']) for b,h in results.items()})\n",
        "results_df.to_csv(f\"{OUT_DIR}/training_results.csv\", index=False)\n",
        "print(f\"📊 Training results saved to {OUT_DIR}/training_results.csv\")\n",
        "\n",
        "df = pd.read_csv(f\"{OUT_DIR}/train.csv\")\n",
        "label2idx = {l: i for i, l in enumerate(sorted(df['label'].unique()))}\n",
        "idx2label = {i: l for l, i in label2idx.items()}\n",
        "num_classes = len(label2idx)"
      ],
      "metadata": {
        "id": "bI4ztVhLK6r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Comparision Between Model**"
      ],
      "metadata": {
        "id": "CNKluPcpPnOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = list(results.keys())\n",
        "val_acc_scores = [h['val_acc'][-1] for h in results.values()]\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(names, val_acc_scores, color=\"green\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Model Comparison on Tomato Leaf Disease Dataset\")\n",
        "plt.show()\n",
        "print(\"Final Validation Accuracies:\", dict(zip(names,val_acc_scores)))"
      ],
      "metadata": {
        "id": "lKMJ9PIvK7g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Remedies Knowledge Base**"
      ],
      "metadata": {
        "id": "gRC1EYZfPwiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remedies = {\n",
        "    \"Bacterial spot\": {\"description\": \"Caused by Xanthomonas spp. Small, water-soaked spots that turn dark and necrotic.\",\n",
        "                       \"actions\": [\"Apply copper-based bactericides.\",\"Avoid overhead irrigation and overcrowding.\",\"Rotate crops and use certified disease-free seeds.\"]},\n",
        "    \"Early blight\": {\"description\": \"Caused by Alternaria solani. Brown concentric rings on older leaves.\",\n",
        "                     \"actions\": [\"Remove and destroy infected leaves.\",\"Spray fungicides containing chlorothalonil or mancozeb.\",\"Rotate crops with non-solanaceous plants.\"]},\n",
        "    \"Healthy\": {\"description\": \"No visible disease symptoms detected.\",\n",
        "                \"actions\": [\"Maintain good irrigation and fertilization.\",\"Regularly monitor for early disease symptoms.\",\"Practice crop rotation for long-term health.\"]},\n",
        "    \"Late blight\": {\"description\": \"Caused by Phytophthora infestans. Water-soaked, greasy lesions spreading rapidly.\",\n",
        "                    \"actions\": [\"Immediately remove and destroy infected plants.\",\"Apply fungicides like metalaxyl or mancozeb.\",\"Avoid overhead watering, improve air circulation.\"]},\n",
        "    \"Leaf Mold\": {\"description\": \"Caused by Passalora fulva. Yellow spots on upper surface, olive-green mold beneath.\",\n",
        "                   \"actions\": [\"Improve ventilation, reduce humidity in greenhouses.\",\"Apply sulfur or copper-based fungicides.\",\"Remove plant debris after harvest.\"]},\n",
        "    \"Powdery Mildew\": {\"description\": \"White powdery fungal growth on leaf surfaces.\",\n",
        "                       \"actions\": [\"Spray sulfur-based fungicides or potassium bicarbonate.\",\"Ensure proper spacing and ventilation.\",\"Use resistant varieties when available.\"]},\n",
        "    \"Septoria leaf spot\": {\"description\": \"Caused by Septoria lycopersici. Small circular spots with dark borders.\",\n",
        "                           \"actions\": [\"Prune lower leaves to improve airflow.\",\"Apply fungicides containing chlorothalonil.\",\"Disinfect tools and remove infected debris.\"]},\n",
        "    \"Spider mites Two spotted spider mite\": {\"description\": \"Tiny spider-like pests causing stippling, yellowing, and fine webbing.\",\n",
        "                                             \"actions\": [\"Spray neem oil or insecticidal soap.\",\"Introduce predatory mites (biological control).\",\"Keep plants well-watered; mites thrive in dry conditions.\"]},\n",
        "    \"Target spot\": {\"description\": \"Caused by Corynespora cassiicola. Brown lesions with concentric rings.\",\n",
        "                    \"actions\": [\"Use fungicides (chlorothalonil, copper oxychloride).\",\"Remove infected leaves promptly.\",\"Avoid excessive nitrogen fertilization.\"]},\n",
        "    \"Tomato mosaic virus\": {\"description\": \"Virus causing mottled light/dark green areas, distorted leaves.\",\n",
        "                            \"actions\": [\"Remove and destroy infected plants immediately.\",\"Avoid tobacco handling near plants.\",\"Disinfect tools and hands with 10% bleach.\"]},\n",
        "    \"Tomato yellow leaf curl virus\": {\"description\": \"Transmitted by whiteflies. Causes yellowing, curling, and stunted growth.\",\n",
        "                                      \"actions\": [\"Use insect-proof netting to reduce whitefly exposure.\",\"Apply neem oil or insecticides against whiteflies.\",\"Grow resistant tomato varieties.\"]}\n",
        "}\n",
        "with open(f\"{OUT_DIR}/remedies.json\",\"w\") as f:\n",
        "    json.dump(remedies,f,indent=4)\n",
        "print(\"✅ Remedies knowledge base saved\")"
      ],
      "metadata": {
        "id": "_lyongzSPzSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Prediction & Remedies**"
      ],
      "metadata": {
        "id": "OxCTI4FBP3R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image_with_gradcam(image_path, model_path, backbone, num_classes, label_map, remedy_path=f\"{OUT_DIR}/remedies.json\"):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load remedies\n",
        "    with open(remedy_path) as f:\n",
        "        remedy_db = json.load(f)\n",
        "\n",
        "    # Load model\n",
        "    model = timm.create_model(backbone, pretrained=False, num_classes=num_classes)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    # Preprocess image\n",
        "    T = A.Compose([A.Resize(224,224), A.Normalize(), ToTensorV2()])\n",
        "    img_bgr = cv2.imread(image_path)\n",
        "    img_rgb = img_bgr[:,:,::-1]\n",
        "    x = T(image=img_rgb)['image'].unsqueeze(0).to(device)\n",
        "\n",
        "    # Grad-CAM Hook\n",
        "    gradients = []\n",
        "    activations = []\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        gradients.append(grad_output[0])\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        activations.append(output)\n",
        "\n",
        "    # Register hook on last conv layer\n",
        "    last_conv = None\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, (nn.Conv2d)):\n",
        "            last_conv = m\n",
        "    if last_conv is None: raise ValueError(\"No Conv2d layer found for Grad-CAM\")\n",
        "    last_conv.register_forward_hook(forward_hook)\n",
        "    last_conv.register_backward_hook(backward_hook)\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(x)\n",
        "    probs = torch.nn.functional.softmax(logits, dim=1)[0]\n",
        "    pred_idx = probs.argmax().item()\n",
        "    label = label_map[pred_idx]\n",
        "    confidence = float(probs[pred_idx])\n",
        "    remedy = remedy_db.get(label, {\"description\":\"N/A\",\"actions\":[]})\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    logits[0,pred_idx].backward()\n",
        "    grad = gradients[0].cpu().data.numpy()[0]\n",
        "    act  = activations[0].cpu().data.numpy()[0]\n",
        "\n",
        "    weights = grad.mean(axis=(1,2))\n",
        "    cam = np.zeros(act.shape[1:], dtype=np.float32)\n",
        "    for i,w in enumerate(weights):\n",
        "        cam += w * act[i]\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cv2.resize(cam, (224,224))\n",
        "    cam = cam - cam.min()\n",
        "    cam = cam / cam.max()\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(img_rgb, 0.6, heatmap[:,:,::-1], 0.4, 0)\n",
        "\n",
        "    # Display\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(overlay)\n",
        "    plt.title(f\"Grad-CAM: {label} ({confidence:.2f})\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    return label, confidence, remedy"
      ],
      "metadata": {
        "id": "kFkSa634P8En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Example test**"
      ],
      "metadata": {
        "id": "sOJMNDbVQDY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = \"\"  # Put your test image path\n",
        "best_model = f\"models/resnet50.pt\"\n",
        "\n",
        "label, conf, remedy = predict_image_with_gradcam(test_image, best_model, \"resnet50\", len(label2idx), idx2label)\n",
        "\n",
        "print(\"Prediction:\", label)\n",
        "print(\"Confidence:\", round(conf,3))\n",
        "print(\"Description:\", remedy[\"description\"])\n",
        "print(\"Suggested Actions:\")\n",
        "for act in remedy[\"actions\"]:\n",
        "    print(\"-\", act)"
      ],
      "metadata": {
        "id": "jfiRYeD6QMKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "INIEWotbQWCG"
      }
    }
  ]
}